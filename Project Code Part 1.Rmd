---
title: "Applied Data Science Project 1"
author: "Krista(Ruijing) Zhang"
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
```{r}
library(tm)
library(SnowballC)
library(wordcloud)
library(stringr)
```
```{r}
library(readr)
library(dplyr)
library(janeaustenr)
library(tidytext)

phi <- read_csv("philosophy_data.csv")
head(phi)
```
```{r}
time<- phi %>%
  distinct(original_publication_date)
max(time)
```

```{r}
many1<- phi %>%
  filter(original_publication_date < 200)

dim(many1)

many2<- phi %>%
  filter(original_publication_date > 200 & original_publication_date <1900)
dim(many2)

many3<- phi %>%
  filter(original_publication_date == 1900 | original_publication_date >1900)
dim(many3)

dim(phi)
```

## A period
```{r}
data <- phi%>% 
  filter(original_publication_date < 200)
dim(data)
#View(data)

#1. Switch to lower case 2. Remove numbers 3. Remove punctuation marks and stopwords 4. Remove extra whitespaces
data_corpus = Corpus(VectorSource(data$sentence_lowered))
data_corpus = tm_map(data_corpus, removeNumbers)
data_corpus = tm_map(data_corpus, removePunctuation)
data_corpus = tm_map(data_corpus, removeWords, c("the", "and", stopwords("english")))
data_corpus =  tm_map(data_corpus, stripWhitespace)



inspect(data_corpus[1])
data_dtm <- DocumentTermMatrix(data_corpus)




#To reduce the dimension of the DTM, we can remove the less frequent terms such that the sparsity is less than 0.95
data_dtm = removeSparseTerms(data_dtm, 0.99)

data_dtm
```


```{r}
findFreqTerms(data_dtm, 1000)
freq = data.frame(sort(colSums(as.matrix(data_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
```


```{r}
data_dtm_tfidf <- DocumentTermMatrix(data_corpus, control = list(weighting = weightTfIdf))
data_dtm_tfidf = removeSparseTerms(data_dtm_tfidf, 0.95)
data_dtm_tfidf
```
```{r}
freq = data.frame(sort(colSums(as.matrix(data_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```

## B period
```{r}
dataA <- phi%>% 
  filter(original_publication_date > 200 & original_publication_date <1900)
dim(dataA)
#View(dataA)
summary(dataA$tokenized_txt)


#1. Switch to lower case 2. Remove numbers 3. Remove punctuation marks and stopwords 4. Remove extra whitespaces
dataA_corpus = Corpus(VectorSource(dataA$sentence_lowered))
dataA_corpus = tm_map(dataA_corpus, removeNumbers)
dataA_corpus = tm_map(dataA_corpus, removePunctuation)
dataA_corpus = tm_map(dataA_corpus, removeWords, c("the", "and", stopwords("english")))
dataA_corpus =  tm_map(dataA_corpus, stripWhitespace)



inspect(dataA_corpus[1])
dataA_dtm <- DocumentTermMatrix(dataA_corpus)


```

```{r}

#To reduce the dimension of the DTM, we can remove the less frequent terms such that the sparsity is less than 0.95
dataA_dtm = removeSparseTerms(dataA_dtm, 0.99)

dataA_dtm
```


```{r}
findFreqTerms(dataA_dtm, 1000)
freq = data.frame(sort(colSums(as.matrix(dataA_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
```


```{r}
dataA_dtm_tfidf <- DocumentTermMatrix(dataA_corpus, control = list(weighting = weightTfIdf))
dataA_dtm_tfidf = removeSparseTerms(dataA_dtm_tfidf, 0.95)
dataA_dtm_tfidf
```
```{r}
freq = data.frame(sort(colSums(as.matrix(dataA_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```

## C period
```{r}
dataB <- phi%>% 
  filter(original_publication_date == 1900 | original_publication_date >1900)
dim(dataB)
#View(dataB)
summary(dataB$tokenized_txt)


dataB_corpus = Corpus(VectorSource(dataB$sentence_lowered))
dataB_corpus = tm_map(dataB_corpus, removeNumbers)
dataB_corpus = tm_map(dataB_corpus, removePunctuation)
dataB_corpus = tm_map(dataB_corpus, removeWords, c("the", "and", stopwords("english")))
dataB_corpus =  tm_map(dataB_corpus, stripWhitespace)



inspect(dataB_corpus[1])
dataB_dtm <- DocumentTermMatrix(dataB_corpus)

dataB_dtm = removeSparseTerms(dataB_dtm, 0.99)

dataB_dtm
```
```{r}
findFreqTerms(dataB_dtm, 1000)
freq = data.frame(sort(colSums(as.matrix(dataB_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
```

```{r}
dataB_dtm_tfidf <- DocumentTermMatrix(dataB_corpus, control = list(weighting = weightTfIdf))
dataB_dtm_tfidf = removeSparseTerms(dataB_dtm_tfidf, 0.95)
dataB_dtm_tfidf
```

```{r}
freq = data.frame(sort(colSums(as.matrix(dataB_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```



## Words distribution of authors in A period


```{r}
many1<- phi %>%
  filter(original_publication_date < 200)%>%
  group_by(author) %>%
  summarise(Total_word=sum(sentence_length))
many1

library(ggplot2)

ggplot(data=many1, aes(x=
author, y=Total_word)) +
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()
```



## Words distribution of authors in B period


```{r}
many2<- phi %>%
  filter(original_publication_date > 200 & original_publication_date <1900)%>%
  group_by(author) %>%
  summarise(Total_word=sum(sentence_length))

many2
ggplot(data=many2, aes(x=
author, y=Total_word)) +
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()
```





